from ultralytics import YOLO
import face_recognition
import pickle
import cv2
import numpy as np


class VisionSystem:
    """
    æ•´åˆ YOLOï¼ˆå°è±¬åµæ¸¬ï¼‰ + Face Recognitionï¼ˆäººè‡‰è­˜åˆ¥ï¼‰

    æ”¯æ´å…©ç¨®æ¨¡å¼ï¼š
    - æ¨™æº–æ¨¡å¼ (smart_mode=False): æ¯å¹€éƒ½è™•ç†ï¼Œé©åˆå¿«é€Ÿç§»å‹•å ´æ™¯
    - æ™ºèƒ½æ¨¡å¼ (smart_mode=True): å‹•æ…‹åµæ¸¬è§¸ç™¼ï¼ŒTX2 ä¸Š FPS æå‡ 3-5 å€
    """

    def __init__(
        self,
        yolo_weights='best_pig_model.pt',
        face_encoding_file='owner_face.pkl',
        yolo_conf=0.5,
        face_tolerance=0.5,
        smart_mode=False,
        motion_threshold=500
    ):
        """
        åˆå§‹åŒ–è¦–è¦ºç³»çµ±

        Args:
            yolo_weights: YOLO æ¬Šé‡æª”è·¯å¾‘
            face_encoding_file: ä¸»äººäººè‡‰ç‰¹å¾µæª”
            yolo_conf: YOLO ç½®ä¿¡åº¦é–¾å€¼ (é è¨­ 0.5)
            face_tolerance: äººè‡‰è­˜åˆ¥é–¾å€¼ (é è¨­ 0.5ï¼Œè¶Šå°è¶Šåš´æ ¼)
            smart_mode: æ˜¯å¦å•Ÿç”¨æ™ºèƒ½å„ªåŒ–æ¨¡å¼ (é è¨­ False)
            motion_threshold: å‹•æ…‹åµæ¸¬é–¾å€¼ï¼Œåƒ…åœ¨ smart_mode=True æ™‚ä½¿ç”¨ (é è¨­ 500)
        """
        mode_text = "æ™ºèƒ½æ¨¡å¼ï¼ˆå‹•æ…‹è§¸ç™¼ï¼‰" if smart_mode else "æ¨™æº–æ¨¡å¼"
        print("=" * 50)
        print(f"åˆå§‹åŒ– Guardian Eye è¦–è¦ºç³»çµ± - {mode_text}")
        print("=" * 50)

        # è¼‰å…¥ YOLO æ¨¡å‹
        print(f"[1/2] è¼‰å…¥ YOLO æ¨¡å‹: {yolo_weights}")
        try:
            self.yolo = YOLO(yolo_weights)
            self.yolo_conf = yolo_conf
            print("      âœ… YOLO æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        except Exception as e:
            print(f"      âŒ YOLO æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise

        # è¼‰å…¥äººè‡‰è­˜åˆ¥å™¨
        print(f"[2/2] è¼‰å…¥äººè‡‰è­˜åˆ¥å™¨: {face_encoding_file}")
        try:
            with open(face_encoding_file, 'rb') as f:
                self.owner_encodings = pickle.load(f)
            self.face_tolerance = face_tolerance
            print(f"      âœ… å·²è¼‰å…¥ {len(self.owner_encodings)} å€‹ä¸»äººç‰¹å¾µ")
        except Exception as e:
            print(f"      âŒ äººè‡‰ç‰¹å¾µè¼‰å…¥å¤±æ•—: {e}")
            raise

        # æ™ºèƒ½æ¨¡å¼è¨­å®š
        self.smart_mode = smart_mode
        self.motion_threshold = motion_threshold
        self.prev_frame = None

        # å¿«å–ä¸Šä¸€æ¬¡çš„çµæœï¼ˆæ™ºèƒ½æ¨¡å¼ä½¿ç”¨ï¼‰
        self.last_result = {
            'pig_detected': False,
            'pig_confidence': 0.0,
            'pig_bbox': None,
            'person_detected': False,
            'face_result': 'NO_FACE',
            'face_bbox': None,
            'face_confidence': 0.0
        }

        # æ•ˆèƒ½çµ±è¨ˆ
        self.stats = {
            'total_frames': 0,
            'motion_detected': 0,
            'motion_skipped': 0,
            'yolo_runs': 0,
            'face_recognition_runs': 0
        }

        if smart_mode:
            print(f"      âš¡ æ™ºèƒ½æ¨¡å¼å·²å•Ÿç”¨ï¼ˆå‹•æ…‹é–¾å€¼: {motion_threshold}ï¼‰")

        print("=" * 50)
        print("âœ… è¦–è¦ºç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
        print("=" * 50)

    def _detect_motion(self, frame):
        """
        å‹•æ…‹åµæ¸¬ï¼ˆåƒ…åœ¨æ™ºèƒ½æ¨¡å¼ä½¿ç”¨ï¼‰

        Args:
            frame: OpenCV å½±åƒ

        Returns:
            bool: True=æœ‰å‹•éœï¼ŒFalse=ç„¡å‹•éœ
        """
        # è½‰ç°éš
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (21, 21), 0)

        # ç¬¬ä¸€æ¬¡å‘¼å«ï¼Œå»ºç«‹èƒŒæ™¯
        if self.prev_frame is None:
            self.prev_frame = gray
            return True  # ç¬¬ä¸€å¹€é è¨­æœ‰å‹•éœ

        # è¨ˆç®—å·®ç•°
        frame_diff = cv2.absdiff(self.prev_frame, gray)
        thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)[1]
        thresh = cv2.dilate(thresh, None, iterations=2)

        # è¨ˆç®—è®ŠåŒ–é¢ç©
        motion_pixels = cv2.countNonZero(thresh)

        # æ›´æ–°èƒŒæ™¯
        self.prev_frame = gray

        # åˆ¤æ–·æ˜¯å¦æœ‰é¡¯è‘—è®ŠåŒ–
        has_motion = motion_pixels > self.motion_threshold

        return has_motion

    def process_frame(self, frame):
        """
        è™•ç†å–®å¹€å½±åƒï¼ˆæ ¸å¿ƒå‡½æ•¸ï¼‰

        æ™ºèƒ½æ¨¡å¼ï¼šåªåœ¨åµæ¸¬åˆ°å‹•éœæ™‚æ‰è™•ç†
        æ¨™æº–æ¨¡å¼ï¼šæ¯å¹€éƒ½è™•ç†

        Args:
            frame: OpenCV å½±åƒï¼ˆnumpy.ndarray, BGRæ ¼å¼ï¼‰

        Returns:
            dict: {
                'pig_detected': True/False,
                'pig_confidence': 0.0-1.0,
                'pig_bbox': [x1, y1, x2, y2] or None,
                'person_detected': True/False,
                'face_result': 'OWNER'/'STRANGER'/'NO_FACE',
                'face_bbox': [x1, y1, x2, y2] or None,
                'face_confidence': 0.0-1.0
            }
        """
        self.stats['total_frames'] += 1

        # ========== æ™ºèƒ½æ¨¡å¼ï¼šå‹•æ…‹åµæ¸¬è§¸ç™¼ ==========
        if self.smart_mode:
            has_motion = self._detect_motion(frame)

            if not has_motion:
                # æ²’æœ‰å‹•éœï¼Œç›´æ¥è¿”å›ä¸Šæ¬¡çµæœ
                self.stats['motion_skipped'] += 1
                return self.last_result

            # æœ‰å‹•éœï¼Œç¹¼çºŒè™•ç†
            self.stats['motion_detected'] += 1

        # ========== é–‹å§‹è™•ç†å½±åƒ ==========
        result = {
            'pig_detected': False,
            'pig_confidence': 0.0,
            'pig_bbox': None,
            'person_detected': False,
            'face_result': 'NO_FACE',
            'face_bbox': None,
            'face_confidence': 0.0
        }

        # ========== Part 1: YOLO ç‰©é«”åµæ¸¬ ==========
        self.stats['yolo_runs'] += 1
        yolo_results = self.yolo(frame, conf=self.yolo_conf, verbose=False)

        for box in yolo_results[0].boxes:
            class_id = int(box.cls[0])
            confidence = float(box.conf[0])
            bbox = box.xyxy[0].cpu().numpy().astype(int).tolist()

            # é¡åˆ¥ 0 = pigï¼ˆå› ç‚ºæˆ‘å€‘åªè¨“ç·´äº†ä¸€å€‹é¡åˆ¥ï¼‰
            if class_id == 0:
                result['pig_detected'] = True
                result['pig_confidence'] = confidence
                result['pig_bbox'] = bbox
                print(f"ğŸ· åµæ¸¬åˆ°å°è±¬ï¼ç½®ä¿¡åº¦ï¼š{confidence:.2%} ä½ç½®ï¼š{bbox}")
                break  # åªè™•ç†ç¬¬ä¸€å€‹å°è±¬

        # ========== Part 2: äººè‡‰è­˜åˆ¥ ==========
        self.stats['face_recognition_runs'] += 1
        # è½‰æ›é¡è‰²ï¼ˆOpenCV æ˜¯ BGRï¼Œface_recognition è¦ RGBï¼‰
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # åµæ¸¬äººè‡‰ä½ç½®
        face_locations = face_recognition.face_locations(rgb_frame)

        if len(face_locations) > 0:
            result['person_detected'] = True
            face_loc = face_locations[0]  # åªè™•ç†ç¬¬ä¸€å¼µè‡‰

            # è½‰æ›åº§æ¨™æ ¼å¼ï¼ˆtop, right, bottom, left â†’ x1, y1, x2, y2ï¼‰
            top, right, bottom, left = face_loc
            result['face_bbox'] = [left, top, right, bottom]

            # æå–ç‰¹å¾µä¸¦æ¯”å°
            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)

            if len(face_encodings) > 0:
                encoding = face_encodings[0]

                # èˆ‡ä¸»äººç‰¹å¾µæ¯”å°
                matches = face_recognition.compare_faces(
                    self.owner_encodings,
                    encoding,
                    tolerance=self.face_tolerance
                )

                distances = face_recognition.face_distance(self.owner_encodings, encoding)

                if True in matches:
                    best_idx = np.argmin(distances)
                    confidence = 1 - distances[best_idx]
                    result['face_result'] = 'OWNER'
                    result['face_confidence'] = confidence
                    print(f"ğŸ‘¤ è­˜åˆ¥ç‚ºä¸»äººï¼ˆç›¸ä¼¼åº¦ï¼š{confidence:.2%}ï¼‰")
                else:
                    min_distance = np.min(distances)
                    confidence = 1 - min_distance
                    result['face_result'] = 'STRANGER'
                    result['face_confidence'] = confidence
                    print(f"âš ï¸  è­˜åˆ¥ç‚ºé™Œç”Ÿäººï¼ˆæœ€é«˜ç›¸ä¼¼åº¦ï¼š{confidence:.2%}ï¼‰")

        # æ›´æ–°å¿«å–ï¼ˆæ™ºèƒ½æ¨¡å¼ä½¿ç”¨ï¼‰
        if self.smart_mode:
            self.last_result = result

        return result

    def draw_results(self, frame, result):
        """
        åœ¨å½±åƒä¸Šç¹ªè£½åµæ¸¬çµæœ

        Args:
            frame: åŸå§‹å½±åƒ
            result: process_frame() çš„è¿”å›å€¼

        Returns:
            ç¹ªè£½å¾Œçš„å½±åƒ
        """
        output = frame.copy()

        # ç¹ªè£½å°è±¬æ¡†
        if result['pig_detected']:
            x1, y1, x2, y2 = result['pig_bbox']
            cv2.rectangle(output, (x1, y1), (x2, y2), (0, 0, 255), 3)  # ç´…è‰²
            label = f"Pig {result['pig_confidence']:.2%}"
            cv2.putText(output, label, (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

        # ç¹ªè£½äººè‡‰æ¡†
        if result['person_detected']:
            x1, y1, x2, y2 = result['face_bbox']

            # æ ¹æ“šèº«ä»½é¸æ“‡é¡è‰²
            if result['face_result'] == 'OWNER':
                color = (0, 255, 0)  # ç¶ è‰²
                label = f"Owner {result['face_confidence']:.2%}"
            else:
                color = (255, 0, 0)  # è—è‰²
                label = f"Stranger {result['face_confidence']:.2%}"

            cv2.rectangle(output, (x1, y1), (x2, y2), color, 2)
            cv2.putText(output, label, (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        return output

    def get_stats(self):
        """
        å–å¾—æ•ˆèƒ½çµ±è¨ˆè³‡è¨Š

        Returns:
            dict: æ•ˆèƒ½çµ±è¨ˆ
        """
        total = self.stats['total_frames']
        if total == 0:
            return self.stats

        skip_ratio = (self.stats['motion_skipped'] / total) * 100 if self.smart_mode else 0
        process_ratio = (self.stats['motion_detected'] / total) * 100 if self.smart_mode else 100

        return {
            **self.stats,
            'skip_ratio': skip_ratio,
            'process_ratio': process_ratio
        }

    def print_stats(self):
        """å°å‡ºæ•ˆèƒ½çµ±è¨ˆ"""
        stats = self.get_stats()

        print("\n" + "=" * 60)
        print("ğŸ“Š æ•ˆèƒ½çµ±è¨ˆ")
        print("=" * 60)
        print(f"æ¨¡å¼ï¼š          {'æ™ºèƒ½æ¨¡å¼' if self.smart_mode else 'æ¨™æº–æ¨¡å¼'}")
        print(f"ç¸½å¹€æ•¸ï¼š        {stats['total_frames']}")

        if self.smart_mode:
            print(f"åµæ¸¬åˆ°å‹•éœï¼š    {stats['motion_detected']} ({stats.get('process_ratio', 0):.1f}%)")
            print(f"è·³éè™•ç†ï¼š      {stats['motion_skipped']} ({stats.get('skip_ratio', 0):.1f}%)")

        print(f"YOLO åŸ·è¡Œæ¬¡æ•¸ï¼š {stats['yolo_runs']}")
        print(f"äººè‡‰è­˜åˆ¥æ¬¡æ•¸ï¼š  {stats['face_recognition_runs']}")
        print("=" * 60)

        if self.smart_mode:
            print(f"âš¡ è·³éç‡ï¼š{stats.get('skip_ratio', 0):.1f}% (è¶Šé«˜è¶Šçœé›»)")
            print("=" * 60)


# ========== æ¸¬è©¦ç¨‹å¼ ==========
if __name__ == "__main__":
    import sys
    import time

    print("\n" + "=" * 60)
    print("Guardian Eye è¦–è¦ºç³»çµ±æ¸¬è©¦")
    print("=" * 60 + "\n")

    # è©¢å•ä½¿ç”¨è€…é¸æ“‡æ¨¡å¼
    print("è«‹é¸æ“‡æ¸¬è©¦æ¨¡å¼ï¼š")
    print("  [1] æ¨™æº–æ¨¡å¼ï¼ˆæ¯å¹€éƒ½è™•ç†ï¼‰")
    print("  [2] æ™ºèƒ½æ¨¡å¼ï¼ˆå‹•æ…‹è§¸ç™¼ï¼ŒTX2 æ¨è–¦ï¼‰")
    choice = input("\nè«‹è¼¸å…¥ (1 æˆ– 2ï¼Œé è¨­ 1): ").strip() or "1"

    smart_mode = (choice == "2")
    motion_threshold = 500

    if smart_mode:
        threshold_input = input(f"å‹•æ…‹é–¾å€¼ (é è¨­ {motion_threshold}): ").strip()
        if threshold_input:
            motion_threshold = int(threshold_input)

    # åˆå§‹åŒ–ç³»çµ±
    vision = VisionSystem(
        yolo_weights='best_pig_model.pt',
        face_encoding_file='owner_face.pkl',
        yolo_conf=0.5,
        face_tolerance=0.5,
        smart_mode=smart_mode,
        motion_threshold=motion_threshold
    )

    print("\né–‹å•Ÿæ”å½±æ©Ÿ...")
    print("æ“ä½œèªªæ˜ï¼š")
    print("  - æŒ‰ 'q' é€€å‡º")
    print("  - æŒ‰ 's' é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š")
    print("  - æŒ‰ 'c' å„²å­˜ç•¶å‰ç•«é¢")
    print("=" * 60 + "\n")

    # é–‹å•Ÿæ”å½±æ©Ÿï¼ˆé™ä½è§£æåº¦ä»¥æå‡æ•ˆèƒ½ï¼‰
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    if not cap.isOpened():
        print("âŒ ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿï¼")
        print("æç¤ºï¼š")
        print("  1. ç¢ºèªæ”å½±æ©Ÿå·²é€£æ¥")
        print("  2. å˜—è©¦æ”¹ç”¨ cap = cv2.VideoCapture(1)")
        exit()

    # FPS è¨ˆç®—
    fps_start_time = time.time()
    fps_frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ ç„¡æ³•è®€å–å½±åƒ")
            break

        fps_frame_count += 1

        # è™•ç†ç•¶å‰å¹€
        result = vision.process_frame(frame)

        # ç¹ªè£½çµæœ
        output = vision.draw_results(frame, result)

        # è¨ˆç®— FPS
        elapsed_time = time.time() - fps_start_time
        if elapsed_time > 0:
            fps = fps_frame_count / elapsed_time
        else:
            fps = 0

        # é¡¯ç¤ºæ•ˆèƒ½è³‡è¨Š
        stats = vision.get_stats()

        info_line1 = f"FPS: {fps:.1f} | Frames: {stats['total_frames']}"
        cv2.putText(output, info_line1, (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        if smart_mode:
            skip_ratio = stats.get('skip_ratio', 0)
            info_line2 = f"Processed: {stats['motion_detected']} | Skipped: {stats['motion_skipped']} ({skip_ratio:.1f}%)"
            cv2.putText(output, info_line2, (10, 60),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # é¡¯ç¤º
        cv2.imshow('Guardian Eye Vision System', output)

        # æŒ‰éµè™•ç†
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            print("\nä½¿ç”¨è€…é€€å‡º")
            break
        elif key == ord('s'):
            vision.print_stats()
        elif key == ord('c'):
            filename = f"capture_{stats['total_frames']}.jpg"
            cv2.imwrite(filename, output)
            print(f"ğŸ“¸ å·²å„²å­˜: {filename}")

    # æœ€çµ‚çµ±è¨ˆ
    vision.print_stats()

    cap.release()
    cv2.destroyAllWindows()
    print("\nè¦–è¦ºç³»çµ±å·²é—œé–‰")
